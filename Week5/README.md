# ğŸš€ Week 5 â€“ Deep Learning Foundations (Days 31â€“37)

Week 5 marks the transition from **Machine Learning â†’ Deep Learning**.
In this week, I learned the **core concepts of neural networks** and built my **first real Deep Learning project** using the **MNIST handwritten digit dataset**.

---

# ğŸ§  Topics Covered

## ğŸ”¹ Day 31 â€“ Perceptron

* Basic **single-neuron model** for binary classification
* Linear decision boundary
* Limitation: cannot solve **non-linear problems (XOR)**

ğŸ“Œ Outcome: Understanding the **foundation of neural networks**.

---

## ğŸ”¹ Day 32 â€“ Artificial Neural Networks (ANN)

* Multi-layer structure:

  * Input layer
  * Hidden layer(s)
  * Output layer
* Hidden layers enable **non-linear learning**
* ANN can solve **complex real-world patterns**.

ğŸ“Œ Outcome: Clear understanding of **deep learning architecture**.

---

## ğŸ”¹ Day 33 â€“ Backpropagation

* Training loop:

  1. Forward propagation
  2. Loss calculation
  3. Gradient computation
  4. Weight update using **gradient descent**
* Concepts:

  * Learning rate
  * Vanishing gradient problem

ğŸ“Œ Outcome: Learned **how neural networks actually learn**.

---

## ğŸ”¹ Day 34 â€“ Activation Functions

* **ReLU** â†’ hidden layers (most used)
* **Sigmoid** â†’ binary classification output
* **Softmax** â†’ multi-class classification output

ğŸ“Œ Outcome: Choosing the **right activation for each layer**.

---

## ğŸ”¹ Day 35 â€“ Optimizers

* **SGD**, **Momentum**, **Adam** comparison
* Adam provides **fast and stable convergence**
* Importance of **learning rate tuning**.

ğŸ“Œ Outcome: Understanding **efficient neural network training**.

---

# ğŸ§ª Days 36â€“37 â€“ ANN Project: MNIST Digit Classification

## ğŸ“Š Dataset

* **MNIST handwritten digits (0â€“9)**
* 70,000 grayscale images
* Image size: **28Ã—28 pixels**

## âš™ï¸ Steps Performed

* Data normalization (0â€“255 â†’ 0â€“1)
* Image flattening for ANN (28Ã—28 â†’ 784 features)
* ANN architecture with:

  * Dense hidden layers + ReLU
  * Softmax output layer
* Training using:

  * **Adam optimizer**
  * **Sparse categorical cross-entropy loss**
* Model evaluation on test data
* Visualization of predictions and training accuracy.

## ğŸ“ˆ Result

* Achieved **~95%+ test accuracy** using ANN.
* Demonstrated **complete deep learning workflow** from preprocessing â†’ training â†’ evaluation â†’ prediction.

ğŸ“Œ This is my **first Deep Learning project** and a key **portfolio milestone**.

---

# ğŸ› ï¸ Technologies Used

* Python
* NumPy
* Matplotlib
* TensorFlow / Keras
* Jupyter Notebook
* Git & GitHub

---

# ğŸ“‚ Suggested Folder Structure

```
Week5_Deep_Learning/
â”‚
â”œâ”€â”€ Day31_Perceptron/
â”œâ”€â”€ Day32_ANN/
â”œâ”€â”€ Day33_Backpropagation/
â”œâ”€â”€ Day34_Activation_Functions/
â”œâ”€â”€ Day35_Optimizers/
â”œâ”€â”€ Day36_37_MNIST_ANN_Project/
â”‚
â””â”€â”€ README.md
```

---

# ğŸ† Week 5 Outcome

After completing Week 5, I can:

* Explain **neural network fundamentals** clearly
* Understand **backpropagation, activations, and optimizers**
* Build and train an **ANN model from scratch**
* Work with **image datasets in Deep Learning**
* Deliver a **GitHub-ready Deep Learning project**

â¡ This week represents my transition from **ML learner â†’ Deep Learning beginner**.

---

